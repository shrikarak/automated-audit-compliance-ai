{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Pipeline for Automated Audit and Compliance Monitoring\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady. All rights reserved.**\n",
    "\n",
    "This notebook simulates an automated audit pipeline. It uses a set of specialized AI scanners to analyze different types of corporate data (chats, documents, transactions) to proactively detect potential compliance violations, such as insider trading chatter, PII leakage, and anomalous financial transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q spacy scikit-learn pandas\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Simulation\n",
    "We'll create a diverse set of simulated data sources that our audit scanners will analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: Internal Chat Logs\n",
    "chat_logs = [\n",
    "    {'msg_id': 'C01', 'text': 'Remember to submit your TPS reports by Friday.'},\n",
    "    {'msg_id': 'C02', 'text': 'Has anyone seen the latest numbers? They look good.'},\n",
    "    {'msg_id': 'C03', 'text': \"Don't tell anyone, but the quarterly earnings are going to miss estimates badly.\"},\n",
    "    {'msg_id': 'C04', 'text': 'The deal with Acme Corp is almost final. Huge news coming next week.'}\n",
    "]\n",
    "\n",
    "# Source 2: Documents\n",
    "documents = {\n",
    "    'doc_001.txt': 'Project Phoenix - Marketing Plan. This is a public-facing document.',\n",
    "    'doc_002.txt': 'Project Phoenix - Internal Notes. All good, but a customer, John Doe, needs his password reset. His email is john.d@test.com.'\n",
    "}\n",
    "\n",
    "# Source 3: Financial Transactions\n",
    "def generate_transactions(n_normal=100, n_anomalies=5):\n",
    "    np.random.seed(42)\n",
    "    normal = pd.DataFrame({\n",
    "        'amount': np.random.uniform(100, 5000, n_normal),\n",
    "        'hour_of_day': np.random.randint(9, 18, n_normal)\n",
    "    })\n",
    "    anomalies = pd.DataFrame({\n",
    "        'amount': np.random.uniform(50000, 100000, n_anomalies),\n",
    "        'hour_of_day': np.random.randint(0, 6, n_anomalies)\n",
    "    })\n",
    "    return pd.concat([normal, anomalies]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "transactions_df = generate_transactions()\n",
    "\n",
    "print(\"Simulated data sources are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Compliance Scanner Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatComplianceScanner:\n",
    "    def __init__(self):\n",
    "        self.sensitive_keywords = ['earnings', 'merger', 'acquisition', 'confidential', 'secret', 'deal']\n",
    "    \n",
    "    def scan(self, logs):\n",
    "        findings = []\n",
    "        for log in logs:\n",
    "            for keyword in self.sensitive_keywords:\n",
    "                if re.search(f'\\\\b{keyword}\\\\b', log['text'], re.IGNORECASE):\n",
    "                    findings.append(f\"[MEDIUM] Chat Violation: Found sensitive keyword '{keyword}' in message ID {log['msg_id']}.\")\n",
    "                    break # Avoid multiple flags for the same message\n",
    "        return findings\n",
    "\n",
    "class DocumentPiiScanner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        patterns = [\n",
    "            {\"label\": \"EMAIL\", \"pattern\": [{\\"TEXT\": {\"REGEX\": \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"}}]}\n",
    "        ]\n",
    "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        ruler.add_patterns(patterns)\n",
    "    \n",
    "    def scan(self, docs):\n",
    "        findings = []\n",
    "        for doc_name, text in docs.items():\n",
    "            doc = self.nlp(text)\n",
    "            pii_labels = [ent.label_ for ent in doc.ents if ent.label_ in [\"PERSON\", \"EMAIL\"]]\n",
    "            if pii_labels:\n",
    "                findings.append(f\"[HIGH] PII Leakage: Found entities {list(set(pii_labels))} in document '{doc_name}'.\")\n",
    "        return findings\n",
    "\n",
    "class TransactionAnomalyScanner:\n",
    "    def __init__(self):\n",
    "        self.model = IsolationForest(contamination='auto', random_state=42)\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def train(self, normal_transactions):\n",
    "        print(\"Training transaction anomaly model on normal data...\")\n",
    "        self.model.fit(normal_transactions)\n",
    "        self.is_trained = True\n",
    "        \n",
    "    def scan(self, all_transactions):\n",
    "        if not self.is_trained:\n",
    "            return [\"[ERROR] Transaction scanner has not been trained.\"]\n",
    "        predictions = self.model.predict(all_transactions)\n",
    "        anomalous_indices = np.where(predictions == -1)[0]\n",
    "        findings = [f\"[MEDIUM] Transaction Anomaly: Flagged transaction index {i} for unusual amount/time.\" for i in anomalous_indices]\n",
    "        return findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Audit Orchestrator\n",
    "This component initializes all scanners, runs the audit across all data sources, and consolidates the findings into a single report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuditOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.chat_scanner = ChatComplianceScanner()\n",
    "        self.doc_scanner = DocumentPiiScanner()\n",
    "        self.tx_scanner = TransactionAnomalyScanner()\n",
    "        \n",
    "    def run_full_audit(self, chats, docs, transactions):\n",
    "        print(\"--- Starting Automated Compliance Audit ---\\n\")\n",
    "        all_findings = []\n",
    "        \n",
    "        # 1. Scan Chats\n",
    "        print(\"Scanning internal chat logs...\")\n",
    "        chat_findings = self.chat_scanner.scan(chats)\n",
    "        all_findings.extend(chat_findings)\n",
    "        \n",
    "        # 2. Scan Documents\n",
    "        print(\"Scanning documents for PII leakage...\")\n",
    "        doc_findings = self.doc_scanner.scan(docs)\n",
    "        all_findings.extend(doc_findings)\n",
    "        \n",
    "        # 3. Scan Transactions\n",
    "        print(\"Scanning financial transactions for anomalies...\")\n",
    "        # In a real scenario, you'd train on a large, known-good historical dataset.\n",
    "        # Here, we'll train on the first 80 'normal' records for demonstration.\n",
    "        self.tx_scanner.train(transactions.head(80))\n",
    "        tx_findings = self.tx_scanner.scan(transactions)\n",
    "        all_findings.extend(tx_findings)\n",
    "        \n",
    "        # 4. Generate Final Report\n",
    "        print(\"\\n--- Audit Complete. Generating Report. ---\\n\")\n",
    "        print(\"********************************************\")\n",
    "        print(\"*** AUTOMATED COMPLIANCE AUDIT REPORT ***\")\n",
    "        print(\"********************************************\")\n",
    "        if not all_findings:\n",
    "            print(\"No compliance violations were detected.\")\n",
    "        else:\n",
    "            for finding in sorted(all_findings, reverse=True):\n",
    "                print(f\"- {finding}\")\n",
    "        print(\"********************************************\")\n",
    "\n",
    "# Run the audit\n",
    "orchestrator = AuditOrchestrator()\n",
    "orchestrator.run_full_audit(chat_logs, documents, transactions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
